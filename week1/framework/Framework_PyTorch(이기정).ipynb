{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IabIA345uW2s"
   },
   "source": [
    "## 코드\n",
    "### 아래에 정리 후 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwKyMDKxuDlM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt1H5o-poBfF"
   },
   "source": [
    "# 1. Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1X8mpttn-Vi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transfroms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWzCD1LRn-vT"
   },
   "source": [
    "# 2. use Gpu or Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YfkyOQioSBj"
   },
   "outputs": [],
   "source": [
    "# gpu를 사용할 수 있는지 확인해주는 API\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "print(device + \" is available\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ_MTPYyoUbo"
   },
   "source": [
    "# 3. Hyperparmeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQee8cNioUjL"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDVGW05NoUri"
   },
   "source": [
    "# 4. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hn6FU1proUxO"
   },
   "outputs": [],
   "source": [
    "# MNIST dataset download API for train\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() \n",
    "    ])\n",
    ")\n",
    "# MNIST dataset download API for test\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# dataset을 batchsize로 나누어주고 shuffle을 하거나 여러가지 customize가능한 API\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    " \n",
    "examples = enumerate(train_set)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccwsHldkoU3w"
   },
   "source": [
    "# 5. Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN9BQMZJoU8b"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self): \n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # input_channel = 1, output_channel = 10, kernel size = 5, stribe = 1인 convolution연산 api\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) \n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False) # random하게 2d 입력에 대해서 channel을 끊어주는 연산을 해주는 API\n",
    "        self.mp = nn.MaxPool2d(2) # kernel_size=(2,2), stride=2, sliding window방식으로 max값으로 축소해주는 API\n",
    "        self.fc1 = nn.Linear(320,100)  # input=320->output=100 인 Perecptron Layer API\n",
    "        self.fc2 = nn.Linear(100,10) \n",
    "\n",
    "  def forward(self, x):\n",
    "        x = F.relu(self.mp(self.conv1(x))) # max(0,x) 연산을 해주는 RELU API\n",
    "        x = F.relu(self.mp(self.conv2(x))) \n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc1(x) \n",
    "        x = self.fc2(x) \n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35KJ9PP9oVCI"
   },
   "source": [
    "# 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aJMrF54oVGV"
   },
   "outputs": [],
   "source": [
    " \n",
    "model = ConvNet().to(device) \n",
    "criterion = nn.CrossEntropyLoss().to(device)   # CrossEntropyLoss을 연산해주는 API\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)   # Adam optimizer API\n",
    " \n",
    "for epoch in range(epochs): \n",
    "    avg_cost = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        #모든 model의 gradient 값을 0으로 설정 - 초기화 이유 : 이전 grad영향을 없애기 위해서\n",
    "        hypothesis = model(data)\n",
    "        cost = criterion(hypothesis, target) \n",
    "        # grad을 계산하는 API\n",
    "        cost.backward()\n",
    "        # 계산된 grad을 가지고 파라미터값을 수정해주는 API\n",
    "        optimizer.step() \n",
    "        avg_cost += cost / len(train_loader) \n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bxJSYFToVLy"
   },
   "source": [
    "# 7.  Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yzDsZ9roVQM"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad(): \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(data)\n",
    "        preds = torch.max(out.data, 1)[1] \n",
    "        total += len(target) \n",
    "        correct += (preds==target).sum().item() \n",
    "        \n",
    "    print('Test Accuracy: ', 100.*correct/total, '%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Framework_PyTorch.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
